{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12926c36-4cde-4e0e-8578-9b29e74de30f",
   "metadata": {},
   "source": [
    "## Computational Physics 2 (WS23/24) – Warm-up exercise\n",
    "\n",
    "**Deadline: 31.10.2023 at 23:59**\n",
    "\n",
    "Group: Emmy Noether\n",
    "\n",
    "Students: Janik Rausch (628334), Camilo Tello Breuer (), Ida Wöstheinreich ()\n",
    "\n",
    "You will implement and test two algorithms: **conjugate gradient** and **power method**. We will see in a moment what they are useful for. Fill the notebook following the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9083b4b-b80c-475a-b238-5a5e106e69b3",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Here we load the needed libraries and we initialize the random number generator. **Important**: when using a random number generator, the seed needs to be set only once, preferebly at the beginning of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0080a68-8ad2-4f75-a289-2b738d90e179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random.TaskLocalRNG()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import LinearAlgebra\n",
    "import Random\n",
    "\n",
    "using LinearAlgebra\n",
    "\n",
    "Random.seed!(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e2535-e01a-45c7-abf2-c52b94b8ef36",
   "metadata": {},
   "source": [
    "### Positive-definite matrices\n",
    "\n",
    "Both algorithms will deal with hermitian positive-definite matrices. Recall:\n",
    "\n",
    "- Given a complex square matrix $A$, its hermitian conjugate $A^\\dagger$ is defined as its transposed complex-conjugated, i.e. $(A^\\dagger)_{ij} = (A_{ji})^*$.\n",
    "- A complex square matrix $A$ is said to be hermitian if $A=A^\\dagger$.\n",
    "- An hermitian matrix $A$ is said to be positive-definite if all its eigenvalues are positive.\n",
    "\n",
    "The following function generates and returns a random positive-definite matrix, along with its eigenvactors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af2637e0-5aa5-439e-b312-d235b12cb508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_positive_definite_matrix (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function 'generate_positive_definite_matrix' contructs an NxN positive-definite matrix 'A',\n",
    "# its matrix of eigenvectors and its eigenvalues.\n",
    "#\n",
    "# Input parameters:\n",
    "#    N (integer)        : size of output matrix 'A'\n",
    "#    kappa (double)     : condition number of the output matrix 'A'\n",
    "#                         see https://en.wikipedia.org/wiki/Condition_number#Matrices\n",
    "# Output values: (A, U, evalues)\n",
    "#    A (np.matrix)      : positive-definite NxN matrix with condition number kappa\n",
    "#    U (np.matrix)      : NxN unitary matrix; each column of 'U' is an eigenvector of 'A'\n",
    "#    evalues (np.array) : N-component array with eigenvalues of 'A'\n",
    "\n",
    "function generate_positive_definite_matrix(N::Int64;kappa::Float64=10.)\n",
    "    @assert(N > 1 , \"N=\" * string(N) * \" must be an integer >= 2\")\n",
    "    @assert(kappa > 0. , \"kappa=\" * str(kappa) * \" must be a positive float\")\n",
    "    \n",
    "    rmat = Random.randn(N,N)+1im*Random.randn(N,N)\n",
    "    U , _ = qr(rmat)\n",
    "    evalues = vcat(1. .+ kappa*Random.rand(N-2),[1.;kappa])\n",
    "    A = U*Diagonal(evalues)*U'\n",
    "    \n",
    "    return A, U, evalues\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c2190-88cd-49f7-9bb3-8610ccbb1e0c",
   "metadata": {},
   "source": [
    "### Power method\n",
    "\n",
    "Given a positive-definite matrix $A$, the power method allows to approximate its largest eigenvalue and the corresponding eigenvector with a certain specified tolerance $\\epsilon$. It is an iterative method: a number of steps are repeated cyclically, at each iteration one gets a better approximation of the eigenvalue and eigenvectors, the iteration is stopped when the approximation is good enough. It works as follows:\n",
    "\n",
    "1. Generate a random complex vector $v$ with norm equal to 1.\n",
    "2. Calculate $w=Av$ and $\\mu = \\| w \\|$.\n",
    "3. If $\\| w - \\mu v \\| < \\epsilon$, stop iteration and returns $\\mu$ and $v$ are eigenvalue and eigenvector.\n",
    "4. Replace $v \\leftarrow \\mu^{-1} w$ and repeat from 2.\n",
    "\n",
    "**Task:** Implement the power method within the function ```power_method```, with the following specifications.\n",
    "\n",
    "The *vector* $v$ is not necessarily a one-dimensional array, we want the flexibility to use more abstract vector spaces whose elements are generic $d$-dimensional arrays. In practice, the *vectors* $v$ must be implemented as ```numpy.ndarrays```. In this setup, the squared norm $\\|v\\|^2$ of the *vector* $v$ is given by the sum of the squared absolute value of all elements of $v$. Moreover, the *matrix* $A$ really needs to be thought as a linear function acting on the elements of the abstract vector space.\n",
    "\n",
    "```power_method``` must be a function that takes three inputs:\n",
    "- ```vshape``` is the shape of the elements $v$ of the abstract vector space;\n",
    "- ```apply_A``` is a function that takes the vector $v$ (represented as an instance of ```numpy.ndarrays```) and returns the vector $Av$ (represented as an instance of ```numpy.ndarrays``` with the same shape as $v$);\n",
    "- ```epsilon``` is the tolerance.\n",
    "\n",
    "```power_method``` must return:\n",
    "- the largest eignevalue $\\mu$;\n",
    "- the corresponding eigenvector (represented as an instance of ```numpy.ndarrays``` with the same shape as the input of ```apply_A```);\n",
    "- the number of iterations.\n",
    "\n",
    "A test function is provided below. Your implementation of ```power_method``` needs to pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20fa3eb1-f945-47ec-ab5e-19e13a1fe506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "power_method (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function 'power_method' calculates an approximation of the largest eigenvalue 'mu'\n",
    "# and corresponding eigenvector 'v' of the positive-definite linear map 'A'. The quality\n",
    "# of the approximation is dictated by the tolerance 'epsilon', in the sense that the\n",
    "# approximated eigenvalue and eigenvector satisfy\n",
    "#   | A(v) - mu*v | < epsilon\n",
    "#\n",
    "# The vectors over which 'A' acts are generically d-dimensional arrays. More precisely,\n",
    "# they are instances of 'numpy.ndarray' with shape 'vshape'.\n",
    "#\n",
    "# The linear map 'A' is indirectly provided as a function 'apply_A' which takes a vector\n",
    "# v and returns the vector A(v).\n",
    "#\n",
    "# Input parameters of power_method:\n",
    "#    vshape (tuple of ints) : shape of the arrays over which 'A' acts\n",
    "#    apply_A (function)     : function v -> A(v)\n",
    "#    epsilon (float)        : tolerance\n",
    "# Output values: (mu, v, niters)\n",
    "#    mu (float)             : largest eigenvalue of A\n",
    "#    v (numpy.ndarray)      : corresponding eigenvector\n",
    "#    niters (int)           : number of iterations\n",
    "\n",
    "function power_method(vshape::Tuple{Vararg{Int}},apply_A::Function,epsilon::Float64)\n",
    "    \n",
    "    ### Implement your function here\n",
    "\n",
    "    v = rand(Float64,vshape)\n",
    "    niters = 0; maxiters = 1e5\n",
    "\n",
    "    while niters < maxiters\n",
    "        w = apply_A(v)\n",
    "        global mu = norm(w)\n",
    "        if norm(w-mu*v) < epsilon break end\n",
    "        v = w/mu\n",
    "        niters += 1\n",
    "    end\n",
    "    \n",
    "    return mu, v, niters\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d166b",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "Run the following cell. If the power method is correctly implemented, then the test will pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59ae7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (4,)\tresidue = 4.989364942362995e-9\titerations = 19\tTest passes: true\n",
      "shape = (1, 5)\tresidue = 9.98992743409436e-13\titerations = 366\tTest passes: true\n",
      "shape = (3, 2, 4)\tresidue = 9.412437181408594e-9\titerations = 303\tTest passes: true\n",
      "shape = (5, 2)\tresidue = 9.058975321632323e-13\titerations = 122\tTest passes: true\n"
     ]
    }
   ],
   "source": [
    "function test_power_method()\n",
    "\n",
    "    function test_engine(shape::Tuple{Vararg{Int}},epsilon::Float64)\n",
    "        \n",
    "        N = prod(shape)\n",
    "        A , _ , _ = generate_positive_definite_matrix(N)\n",
    "        \n",
    "        function apply_A(v::Array)\n",
    "            @assert size(v)==shape\n",
    "            return reshape(A*reshape(v,(N,)),shape)\n",
    "        end\n",
    "        \n",
    "        mu , v , niters = power_method(shape,apply_A,epsilon)\n",
    "        delta = apply_A(v) - mu*v\n",
    "        res = norm(delta)\n",
    "        println(\"shape = \" , shape , \"\\tresidue = \" , res , \"\\titerations = \" , niters , \"\\tTest passes: \" , res<=epsilon)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    test_engine((4,),1.e-8)\n",
    "    test_engine((1,5),1.e-12)\n",
    "    test_engine((3,2,4),1.e-8)\n",
    "    test_engine((5,2),1.e-12)\n",
    "end\n",
    "\n",
    "test_power_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6108bf6-eb00-4e81-9486-95fa7713e91d",
   "metadata": {},
   "source": [
    "### Conjugate gradient\n",
    "\n",
    "**Task.**\n",
    "1. Read about the conjugate gradient on Wikipedia.\n",
    "2. Implement the conjugate gradient, using same conventions as for the power method.\n",
    "3. Write a description of the algorithm here (in the same spirit as the description of the power method).\n",
    "4. Run and pass the test provided below.\n",
    "5. Discuss intermediate steps with tutors.\n",
    "\n",
    "**Description.**\n",
    "The conjugate gradient method allows solving the linear system $Ax=b$ in the case of a hermitian, positive-definite matrix $A$. It uses the fact that the solution $x$ is also the unique minimizer of $f(y)=y^\\dag Ay-2y^\\dag b$. The idea is to use gradient descent with the added constraint that subsequent search directions $p_i$ must be conjugate wrt. $A$, meaning $p_i^\\dag Ap_j=0$ for $i\\neq j$.\n",
    "\n",
    "This leads to the following algorithm:\n",
    "\n",
    "1. Start with an initial guess $x_0$ and the search direction $p_0=r_0=b-Ax_0$.\n",
    "2. The next guess is $$x_{k+1}=x_k+\\alpha_k p_k\\quad\\text{with}\\quad\\alpha_k=\\frac{p_k^\\dag r_k}{p_k^\\dag Ap_k}.$$\n",
    "3. The new residual is $r_{k+1}=b-Ax_{k+1}$. If $||r_{k+1}||\\leq\\epsilon||b||$, break the loop.\n",
    "4. The new conjugate search direction is $$p_{k+1}=r_{k+1}-\\sum_{i\\leq k}\\frac{p_i^\\dag Ar_{k+1}}{p_i^\\dag Ap_i}.$$\n",
    "5. Repeat from 2.\n",
    "\n",
    "However, this can be simplified by exploiting that the residuals $r_i$ are pairwise orthogonal (which can be shown by complete induction). This yields modified expressions for $\\alpha_k$ and $p_{k+1}$:\n",
    "\n",
    "$$\\alpha_k=\\frac{r_k^\\dag r_k}{p_k^\\dag Ap_k}\\quad\\text{and}\\quad p_{k+1}=r_{k+1}+\\frac{r_{k+1}^\\dag r_{k+1}}{r_k^\\dag r_k}p_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "389023bc-420a-4cca-8cfc-061f2de871d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conjugate_gradient (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function 'conjugate_gradient' calculates an approximation 'x' of 'A^{-1}(b)', where\n",
    "# 'A' is a positive-definite linear map 'A', and 'b' is a vector in the domain of 'A'.\n",
    "# The quality of the approximation is dictated by the tolerance 'epsilon', in the sense\n",
    "# that the following inequality is satisfied\n",
    "#   | A(x) - b | <= epsilon |b|\n",
    "#\n",
    "# The vectors over which 'A' acts are generically d-dimensional arrays. More precisely,\n",
    "# they are instances of 'numpy.ndarray' with shape 'vshape'.\n",
    "#\n",
    "# The linear map 'A' is indirectly provided as a function 'apply_A' which takes a vector\n",
    "# v and returns the vector A(v).\n",
    "#\n",
    "# Input parameters of power_method:\n",
    "#    apply_A (function)     : function v -> A(v)\n",
    "#    b (numpy.ndarray)      : vector 'b'\n",
    "#    epsilon (float)        : tolerance\n",
    "# Output values: (x, niters)\n",
    "#    x (numpy.ndarray)      : approximation of 'A^{-1}(b)'\n",
    "#    niters (int)           : number of iterations\n",
    "\n",
    "function conjugate_gradient(apply_A::Function,b::Array,epsilon::Float64)\n",
    "\n",
    "    ### Implement your function here\n",
    "\n",
    "    x = rand(Float64,size(b)); r = b - apply_A(x); p = r\n",
    "    niters = 0; maxiters = 1e5\n",
    "    \n",
    "    while niters < maxiters\n",
    "        x = x + dot(r,r) / dot(p,apply_A(p)) * p\n",
    "        r_new = b - apply_A(x)\n",
    "        if norm(r_new) < epsilon*norm(b) break end\n",
    "        p = r_new + dot(r_new,r_new) / dot(r,r) * p\n",
    "        r = r_new \n",
    "        niters += 1\n",
    "    end\n",
    "    \n",
    "    return x, niters\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c74f23d-70f9-437b-8394-4ea77e0c0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (4,)\tresidue = 1.8576197769089357e-13\titerations = 3\tTest passes: true\n",
      "shape = (1, 5)\tresidue = 2.4581080623448042e-15\titerations = 4\tTest passes: true\n",
      "shape = (3, 2, 4)\tresidue = 1.6052642153633876e-8\titerations = 22\tTest passes: true\n",
      "shape = (5, 2)\tresidue = 2.2732477266632935e-15\titerations = 9\tTest passes: true\n"
     ]
    }
   ],
   "source": [
    "function test_conjugate_gradient()\n",
    "\n",
    "    function test_engine(shape::Tuple{Vararg{Int}},epsilon::Float64)\n",
    "        \n",
    "        N = prod(shape)\n",
    "        A , _ , _ = generate_positive_definite_matrix(N)\n",
    "        b = Random.randn(shape)+1im*Random.randn(shape)\n",
    "        \n",
    "        function apply_A(v::Array)\n",
    "            @assert size(v)==shape\n",
    "            return reshape(A*reshape(v,(N,)),shape)\n",
    "        end\n",
    "        \n",
    "        x , niters = conjugate_gradient(apply_A,b,epsilon)\n",
    "        delta = apply_A(x) - b\n",
    "        res = norm(delta)\n",
    "        println(\"shape = \" , shape , \"\\tresidue = \" , res , \"\\titerations = \" , niters , \"\\tTest passes: \" , res<=epsilon*norm(b))\n",
    "    end\n",
    "    \n",
    "    \n",
    "    test_engine((4,),1.e-8)\n",
    "    test_engine((1,5),1.e-12)\n",
    "    test_engine((3,2,4),1.e-8)\n",
    "    test_engine((5,2),1.e-12)\n",
    "end\n",
    "\n",
    "test_conjugate_gradient()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
